WGET（1）GNU Wget WGET（1）



名称
       Wget  - 非交互式网络下载器。

概要
       wget [选项] ... [URL] ...

描述
       GNU Wget是一个免费的实用程序，用于非交互式下载文件
       网络。它支持HTTP，HTTPS和FTP协议，以及
       通过HTTP代理检索。

       Wget是非交互式的，这意味着它可以在后台运行，
       而用户未登录。这允许您开始检索
       并断开与系统的连接，让Wget完成工作。通过
       相比之下，大多数Web浏览器需要不断的用户存在，
       在传输大量数据时，这可能是一个很大的障碍。

       Wget可以跟随HTML，XHTML和CSS页面中的链接来创建本地
       远程网站的版本，完全重新创建目录结构
       原始网站。这有时被称为“递归
       下载。“在这样做时，Wget尊重机器人排除
       标准（/robots.txt）。可以指示Wget转换链接
       下载的文件指向本地文件，以供离线查看。

       Wget专为缓慢或不稳定的网络而设计
       连接;如果由于网络问题导致下载失败，它将保留
       重试，直到检索到整个文件。如果是服务器
       支持regetting，它将指示服务器继续
       从它停止的地方下载。

OPTIONS
   选项语法
       由于Wget使用GNU getopt来处理命令行参数
       选项有一个很长的形式和短的形式。长期选择更多
       方便记住，但需要时间打字。你可以随意混合
       不同的选项样式，或在命令行后指定选项
       参数。因此你可以写：

               wget -r --tries = 10 http://fly.srk.fer.hr/ -o log

       接受参数的选项和参数之间的空格可以
       被省略。而不是-o log，你可以写-olog。

       您可以将几个不需要参数的选项放在一起，
       喜欢：

               wget -drc <URL>

       这完全等同于：

               wget -d -r -c <URL>

       由于可以在参数之后指定选项，因此您可以
       用 - 终止它们。所以以下将尝试下载URL -x，
       报告无法记录：

               wget -o log  -  -x

       接受逗号分隔列表的选项都尊重
       指定空列表的约定清除其值。这可以
       用于清除.wgetrc设置。例如，如果你的.wgetrc
       将“exclude_directories”设置为/ cgi-bin，以下示例将
       首先重置它，然后将其设置为排除/ ~nove和/〜某人。
       您还可以清除.wgetrc中的列表。

               wget -X“-X /〜没人，/〜某人

       大多数不接受参数的选项都是布尔选项，因此命名
       因为他们的状态可以用yes-or-no（“boolean”）捕获
       变量。例如， -  follow-ftp告诉Wget遵循FTP链接
       来自HTML文件，另一方面， -  no-glob告诉它不要
       在FTP URL上执行文件通配。布尔选项是
       肯定的或否定的（以--no开头）。所有这些选择共享
       几个属性。

       除非另有说明，否则假定默认行为是
       与期权完成的相反。例如，记录在案
       --follow-ftp的存在假设默认是不遵循FTP
       来自HTML页面的链接。

       肯定的选项可以通过在--no-之前加上来否定
       选项名称;通过省略--no-可以否定否定选项
       字首。这似乎是多余的 - 如果是默认的
       肯定的选择是不做某事，那为什么要提供一种方法
       明确地把它关掉？但是启动文件实际上可能会改变
       默认。例如，在.wgetrc中使用“follow_ftp = on”会生成Wget
       默认情况下遵循FTP链接，使用--no-follow-ftp是唯一的方法
       从命令行恢复出厂默认设置。

   基本启动选项
       -V
        - 版
           显示Wget的版本。

       -H
        - 救命
           打印描述Wget所有命令行选项的帮助消息。

       -b
        - 背景
           启动后立即转到后台。如果没有输出文件
           通过-o指定，输出重定向到wget-log。

       -e命令
       --execute命令
           执行命令，就好像它是.wgetrc的一部分一样。这样一个命令
           调用将在.wgetrc中的命令之后执行，从而执行
           优先于他们。如果需要指定多个wgetrc
           命令，使用-e的多个实例。

   记录和输入文件选项
       -o logfile
       --output文件=日志文件
           将所有消息记录到logfile。通常会将消息报告给
           标准错误。

       -a logfile
       --append输出=日志文件
           附加到日志文件。这与-o相同，只是它附加到
           logfile而不是覆盖旧的日志文件。如果是logfile的话
           不存在，创建一个新文件。

       -d
       --debug
           打开调试输出，这意味着各种重要的信息
           Wget的开发人员，如果它不能正常工作。你的系统
           管理员可能已选择在没有调试的情况下编译Wget
           支持，在这种情况下-d将无法正常工作。请注意
           使用调试支持进行编译总是安全的 - 用Wget编译
           除非请求，否则调试支持不会打印任何调试信息
           用-d。

       -q
        - 安静
           关闭Wget的输出。

       -v
       --verbose
           使用所有可用数据打开详细输出。默认
           输出很冗长。

       -nv
       --no-详细
           关闭详细而不是完全安静（使用-q），
           这意味着错误消息和基本信息仍然可以得到
           打印。

       --report速=类型
           输出带宽为类型。唯一可接受的值是位。

       -i文件
       --input文件=文件
           从本地或外部文件中读取URL。如果 - 指定为
           文件，URL从标准输入中读取。 （使用./-来阅读
           字面上命名的文件 - 。）

           如果使用此函数，则命令中不需要存在URL
           线。如果命令行和输入中都有URL
           文件，命令行上的那些将是第一个
           检索。如果未指定--force-html，则文件应该
           由一系列URL组成，每行一个。

           但是，如果指定--force-html，则会考虑该文档
           作为HTML。在这种情况下，您可能遇到相关链接问题，
           你可以通过添加“<base href =”url“>”来解决这个问题
           文档或在命令行上指定--base = url。

           如果文件是外部文件，则文档将自动生成
           如果Content-Type与text / html匹配，则视为html。
           此外，文件的位置将隐式用作基础
           href如果没有指定。

       -F
       --force-HTML
           从文件中读取输入时，强制将其视为HTML
           文件。这使您可以从现有链接中检索相对链接
           通过将“<base href =”url“>”添加到本地磁盘上的HTML文件
           HTML，或使用--base命令行选项。

       -B URL
       --base = URL
           使用URL作为参考点，何时解析相对链接
           从通过-i /  -  input-file指定的HTML文件中读取链接
           选项（与--force-html一起使用，或者当输入文件是
           从服务器远程获取，将其描述为HTML）。这是
           相当于HTML输入文件中存在“BASE”标记，
           使用URL作为“href”属性的值。

           例如，如果您为URL指定http：//foo/bar/a.html，和
           Wget从输入文件中读取../baz/b.html，它将被解析
           到http：//foo/baz/b.html。

       --config = FILE
           指定要使用的启动文件的位置。

   下载选项
       --bind地址= ADDRESS
           进行客户端TCP / IP连接时，绑定到本地的ADDRESS
           机。 ADDRESS可以指定为主机名或IP地址。
           如果您的计算机绑定到多个IP，则此选项非常有用。

       -t号码
       --tries =数
           设置重试次数为number。指定0或inf表示无限
           重试。默认值是重试20次，但是除外
           致命错误，如“连接拒绝”或“未找到”（404），其中
           没有重试。

       -O文件
       --output文档=文件
           文档不会写入相应的文件，而是全部
           将被连接在一起并写入文件。如果 - 用作
           文件，文件将打印到标准输出，禁用链接
           转换。 （使用./-打印到名为 - 的文件。）

           使用-O并不意味着简单地“使用名称文件
           URL中的那个;“相反，它类似于shell
           重定向：wget -O文件http：// foo的工作方式与wget类似
           -O  -  http：// foo> file;文件将被立即截断，并且全部
           下载的内容将写在那里。

           因此，不支持-N（用于时间戳检查）
           与-O组合：因为文件总是新创建的，所以它会
           总是有一个非常新的时间戳。如果是这样，将发出警告
           使用组合。

           同样，使用-r或-p和-O可能无法正常工作：Wget
           不会只将第一个文件下载到文件然后下载
           休息到他们的正常名称：将放置所有下载的内容
           在文件中。这在1.11版中已禁用，但已恢复
           （有警告）在1.11.2中，因为在某些情况下这是这样的
           行为实际上可以有一些用处。

           请注意，只有在下载时才允许与-k组合使用
           单个文档，因为在这种情况下它只会转换所有文档
           对外部的相对URI; -k对多个URI没有意义
           当他们全部被下载到一个文件; -k可以使用
           仅当输出是常规文件时。

       -nc
       --no-撞
           如果在同一目录中多次下载文件，
           Wget的行为取决于几个选项，包括-nc。在
           在某些情况下，本地文件将被破坏或覆盖，
           重复下载。在其他情况下，它将被保留。

           在没有-N，-nc，-r或-p的情况下运行Wget时，下载相同的
           同一目录中的文件将导致文件的原始副本
           被保留，第二个副本被命名为file.1。如果说
           文件再次下载，第三个副本将命名为file.2，
           等等。 （这也是-nd的行为，即使是-r或-p
           有效。）当指定-nc时，此行为是
           抑制，Wget将拒绝下载较新的文件副本。
           因此，“no-clobber”“实际上是用词不当
           模式---它不会被阻止（作为数字
           后缀已经在防止挫折了，而是
           多个版本保存已被阻止。

           使用-r或-p运行Wget但没有-N，-nd或-nc时，重新运行
           下载文件将导致新副本被覆盖
           老人。添加-nc将阻止此行为，而不是导致
           要保留的原始版本和任何较新的副本
           服务器被忽略。

           使用-N运行Wget时，有或没有-r或-p，决定
           至于是否下载文件的较新副本取决于
           本地和远程时间戳和文件的大小。 -nc可能没有
           与-N同时指定。

           请注意，当指定-nc时，带有后缀.html或。的文件
           .htm将从本地磁盘加载并解析，就好像它们一样
           已从网上检索。

       --backups =备份
           在（编写）文件之前（通过添加）备份现有文件
           .1后缀（VMS上为_1）到文件名。这样的备份文件是
           旋转到.2，.3等等，直到备份（并且超出了备份）。

       -C
        - 继续
           继续获取部分下载的文件。这很有用
           你想完成由前一个实例启动的下载
           Wget，或其他程序。例如：

                   wget -c ftp://sunsite.doc.ic.ac.uk/ls-lR.Z

           如果当前目录中有一个名为ls-lR.Z的文件，则为Wget
           将假设它是远程文件的第一部分，并且
           将要求服务器从偏移量等于继续检索
           到本地文件的长度。

           请注意，如果只是需要，则无需指定此选项
           Wget的当前调用应该重试下载文件
           连接在中途丢失。这是默认值
           行为。 -c仅影响之前开始的下载恢复
           这个Wget的调用，其本地文件仍然存在
           周围。

           没有-c，前面的例子就是下载遥控器
           文件到ls-lR.Z.1，单独留下截断的ls-lR.Z文件。

           从Wget 1.7开始，如果你在非空文件上使用-c，那么
           事实证明服务器不支持继续下载，
           Wget将拒绝从头开始下载，这样就可以了
           有效破坏现有内容。如果你真的想要的话
           下载从头开始，删除文件。

           同样从Wget 1.7开始，如果你在一个文件上使用-c
           与服务器上的大小相同，Wget将拒绝下载
           文件并打印解释性消息。同样的事情发生在
           服务器上的文件比本地小（大概是因为
           自上次下载以来，它在服务器上已更改
           尝试）---因为“继续”没有意义，没有下载
           发生。

           在硬币的另一边，使用-c时，任何文件都是
           在服务器上比在本地更大的将被认为是不完整的
           下载并且只有“（长度（远程） - 长度（本地））”字节
           下载并添加到本地文件的末尾。这个
           在某些情况下可能需要行为 - 例如，你可以
           使用wget -c只下载附加的新部分
           到数据收集或日志文件。

           但是，如果文件在服务器上更大，因为它已经存在
           改变了，而不是仅仅附加到，你最终会得到一个
           乱码文件。 Wget无法验证本地文件是什么
           真的是远程文件的有效前缀。你需要
           将-c与-r结合使用时要特别小心
           因为每个文件都被视为“不完整的下载”
           候选人。

           另一个例子，如果你试图使用，你会得到一个乱码文件
           -c如果你有一个蹩脚的HTTP代理插入一个“转移
           中断“字符串到本地文件。将来a
           可以添加“回滚”选项来处理这种情况。

           请注意，-c仅适用于FTP服务器和HTTP服务器
           支持“Range”标题。

       --progress =类型
           选择您要使用的进度指示器的类型。法律
           指标是“点”和“条”。

           默认情况下使用“bar”指示符。它绘制了ASCII进度
           条形图（a.k.a“温度计”显示）指示状态
           恢复。如果输出不是TTY，将使用“点”栏
           默认情况下。

           使用--progress = dot切换到“点”显示。它跟踪了
           通过在屏幕上打印点来检索，每个点代表一个
           固定数量的下载数据。

           使用虚线检索时，您还可以设置样式
           将类型指定为dot：style。分配不同的样式
           一个点的不同含义。每个点都带有“默认”样式
           代表1K，簇中有10个点，a中有50个点
           线。 “二进制”风格更像“计算机”
           方向--- 8K点，16点簇和每行48点（其中
           制造384K线）。 “mega”风格适合
           下载非常大的文件---每个点代表64K检索，
           一个簇中有八个点，每行有48个点（所以
           每行包含3M）。

           请注意，您可以使用“进度”设置默认样式
           命令.wgetrc。该设置可能会被覆盖
           命令行。例外情况是，当输出不是TTY时，
           “点”进展将优于“酒吧”。强迫吧
           输出，使用--progress = bar：force。

       -N
       --timestamping
           打开时间戳。

       --no使用的服务器，时间戳
           不要通过服务器上的时间戳设置本地文件的时间戳。

           默认情况下，下载文件时，时间戳设置为
           匹配远程文件中的那些。这允许使用
            - 对随后的wget调用进行时间戳。但是，确实如此
           有时候将本地文件的时间戳基于何时有用
           实际下载;为此目的，
            - 已提供--no-use-server-timestamps选项。

       -S
       --server响应
           打印HTTP服务器发送的标头和FTP发送的响应
           服务器。

        - 蜘蛛
           使用此选项调用时，Wget将表现为Web蜘蛛，
           这意味着它不会下载页面，只需检查
           他们在那里。例如，您可以使用Wget来检查您的
           书签：

                   wget --spider --force-html -i bookmarks.html

           这个功能需要更多的工作让Wget接近
           真正的网络蜘蛛的功能。

       -T秒
       --timeout =秒
           将网络超时设置为秒秒。这相当于
           指定--dns-timeout， -  connect-timeout和--read-timeout，
           一切都在同一时间。

           在与网络交互时，Wget可以检查超时和
           如果操作时间太长，则中止操作。这可以防止异常
           喜欢挂读和无限连接。唯一的超时启用
           默认情况下是900秒的读取超时。将超时设置为0
           完全禁用它。除非你知道自己在做什么，否则就是这样
           最好不要更改默认超时设置。

           所有与超时相关的选项都接受十进制值，以及
           亚秒值。例如，0.1秒是合法的（尽管如此）
           不明智的选择超时。亚秒超时对于有用
           检查服务器响应时间或测试网络延迟。

       --dns超时=秒
           将DNS查找超时设置为秒秒。 DNS查找
           不能在指定时间内完成将失败。默认情况下，
           除了实现的DNS查找之外，DNS查找没有超时
           系统库。

       --connect超时=秒
           将连接超时设置为秒秒。 TCP连接
           需要更长时间才能建立中止。默认情况下，没有
           连接超时，而不是系统库实现的超时。

       --read超时=秒
           将读取（和写入）超时设置为秒秒。的时间
           此超时是指空闲时间：如果在下载的任何时候，
           没有收到超过指定秒数的数据，
           读取失败，重新启动下载。此选项不会
           直接影响整个下载的持续时间。

           当然，远程服务器可以选择终止连接
           比这个选项要求更快。默认读取超时为900
           秒。

       --limit率=量
           将下载速度限制为每秒字节数。金额可能是
           用字节表示，千字节用k后缀，或兆字节用
           m后缀。例如， -  limit-rate = 20k将限制
           检索率为20KB / s。无论如何，这都很有用
           原因，你不希望Wget消耗整个可用的
           带宽。

           此选项允许使用十进制数，通常在
           与电源后缀结合;例如， -  limit-rate = 2.5k是
           合法的价值。

           请注意，Wget通过睡眠适当来实现限制
           网络阅读后花费的时间少于
           由费率指定。最终这种策略会导致TCP
           转移到减慢到大约指定的速率。
           但是，这种平衡可能需要一些时间才能实现
           如果限制率不能很好地发挥作用，不要感到惊讶
           小文件。

       -w秒
       --wait =秒
           等待检索之间的指定秒数。使用
           建议使用此选项，因为它会减轻服务器负载
           减少请求的频率。而不是几秒钟，而不是时间
           可以使用“m”后缀在几分钟内指定，以小时为单位使用
           “h”后缀，或使用“d”后缀的天数。

           如果网络，为此选项指定较大的值很有用
           或目标主机已关闭，以便Wget可以等待足够长的时间
           合理地期望在之前修复网络错误
           重试。此函数指定的等待间隔为
           受到“--random-wait”的影响，看哪。

       --waitretry =秒
           如果您不希望Wget在每次检索之间等待，但仅限于
           在重试失败的下载之间，您可以使用此选项。 wget的
           将使用线性退避，在第一次失败后等待1秒
           在给定文件上，然后在第二次失败后等待2秒
           该文件，最多为您指定的最大秒数。

           默认情况下，Wget将假定值为10秒。

       --random等待
           一些网站可以执行日志分析以识别检索
           通过寻找统计上显着的Wget等程序
           请求之间的时间相似。此选项导致
           请求之间的时间在0.5到1.5 *等待秒之间变化，
           where where是使用--wait选项指定的，以便进行掩码
           Wget的存在来自于此类分析。

           2001年出版的一篇专门讨论流行开发的文章
           消费者平台提供了代码来执行此分析
           飞。其作者建议在C类地址级别阻止
           尽管有变化，但确保自动检索程序被阻止
           DHCP提供的地址。

           --random-wait选项的灵感来自于这种不明智的选择
           建议从网站阻止许多不相关的用户
           一个人的行动。

       --no代理
           即使适当的* _proxy环境，也不要使用代理
           变量已定义。

       -Q配额
       --quota =配额
           指定自动检索的下载配额。价值可以是
           以字节（默认），千字节（带k后缀）或
           兆字节（带m后缀）。

           请注意，配额永远不会影响下载单个文件。因此，如果
           你指定wget -Q10k ftp://wuarchive.wustl.edu/ls-lR.gz，所有的
           将下载ls-lR.gz。即使有几个也是如此
           URL在命令行中指定。但是，配额是
           在递归地检索或从输入中检索时受到尊重
           文件。因此你可以安全地输入wget -Q2m -i sites --- download will
           超过配额时中止。

           将quota配置为0或inf可以限制下载配额。

       --no-DNS缓存
           关闭DNS查找的缓存。通常，Wget会记住IP
           解决它从DNS查找，所以它不必重复
           联系DNS服务器以获取相同（通常很小）的主机集
           它从中检索。此缓存仅存在于内存中;一个新的Wget
           运行将再次联系DNS。

           但是，据报道，在某些情况下并非如此
           希望缓存主机名，即使在短时间内
           像Wget一样运行应用程序。使用此选项，Wget会发布一个新选项
           DNS查找（更确切地说，是对“gethostbyname”的新调用或
           “getaddrinfo”）每次建立新连接。请注意
           此选项不会影响可能执行的缓存
           解析库或外部缓存层，如
           NSCD。

           如果你不确切地理解这个选项的作用，你可能会
           不需要它。

       --restrict-文件名=模式
           更改远程URL中找到的字符必须在转换期间进行转义
           生成本地文件名。受限制的字符
           此选项被转义，即替换为％HH，其中HH是
           与限制字符对应的十六进制数字。
           此选项也可用于强制所有按字母顺序排列的案例
           无论是小写还是大写。

           默认情况下，Wget会转义无效或安全的字符
           作为操作系统上文件名的一部分，以及控件
           通常无法打印的字符。此选项很有用
           更改这些默认值，可能是因为您要下载到
           非本机分区，或者因为要禁用转义
           控制字符，或者您想进一步限制字符
           仅限于ASCII值范围内的那些。

           模式是逗号分隔的文本值集。可以接受的
           值是unix，windows，nocontrol，ascii，小写，和
           大写。值unix和windows是互斥的（一个
           将覆盖另一个），小写和大写。那些
           最后是特殊情况，因为它们不会更改字符集
           将被转义，而是强制本地文件路径
           转换为小写或大写。

           当指定“unix”时，Wget将转义字符/和
           控制范围0--31和128--159中的字符。这是
           在类Unix操作系统上默认。

           当给出“windows”时，Wget会转义字符\，|，/，：，？，
           “，*，<，>，以及0--31和0范围内的控制字符
           128--159。除此之外，Windows模式下的Wget使用+代替
           of：以本地文件名分隔主机和端口，并使用@
           代替 ？从中分离文件名的查询部分
           其余的部分。因此，将保存为的URL
           Unix模式下的www.xemacs.org:4300/search.pl?input=blah会是
           在Windows模式下保存为www.xemacs.org+4300/search.pl@input=blah。
           此模式是Windows上的默认模式。

           如果指定nocontrol，则转出控件
           字符也被关闭。这个选项可能有意义
           您正在下载名称包含UTF-8字符的URL
           可以在UTF-8中保存和显示文件名的系统（有些可能
           UTF-8字节序列中使用的字节值属于
           由Wget指定为“控制”的值）。

           ascii模式用于指定值为的任何字节
           超出ASCII字符范围（即大于127）
           应该逃脱。保存文件名时，这可能很有用
           编码与本地使用的编码不匹配。

       -4
       --inet4只
       -6
       --inet6只
           强制连接到IPv4或IPv6地址。使用--inet4-only或
           -4，Wget只会连接到IPv4主机，忽略AAAA记录
           DNS，并拒绝连接到URL中指定的IPv6地址。
           相反，使用--inet6-only或-6，Wget只能连接到IPv6
           主机并忽略A记录和IPv4地址。

           通常不需要任何选项。默认情况下，
           支持IPv6的Wget将使用主机指定的地址族
           DNS记录。如果DNS以IPv4和IPv6地址响应，
           Wget会按顺序尝试它们，直到它找到一个可以连接的方式
           至。 （另请参阅下面的“--prefer-family”选项。）

           这些选项可用于故意强制使用IPv4或
           双系列系统上的IPv6地址族，通常用于辅助
           调试或处理破碎的网络配置。只有一个
           --inet6-only和--inet4-only可以同时指定。
           在没有IPv6支持的情况下编译的Wget中都没有这两个选项。

       --prefer家庭=无/ IPv4 / IPv6双
           如果选择了多个地址，请连接到地址
           首先使用指定的地址族。返回的地址顺序
           默认情况下使用DNS而不进行更改。

           这可以避免虚假错误并在访问时连接尝试
           从IPv4解析为IPv6和IPv4地址的主机
           网络。例如，www.kame.net解析为
           2001：200：0：8002：203：47ff：fea5：3085和203.178.141.194。什么时候
           首选系列为“IPv4”，首先使用IPv4地址;
           当首选系列为“IPv6”时，使用IPv6地址
           第一;如果指定的值为“none”，则返回地址顺序
           通过DNS使用而不做任何更改。

           与-4和-6不同，此选项不会禁止访问任何地址
           系列，它只会改变地址的顺序
           访问。另请注意，此选项执行重新排序
           是稳定的---它不会影响它的地址顺序
           家庭。也就是说，所有IPv4地址的相对顺序和
           在所有情况下，所有IPv6地址都保持不变。

       --retry-connrefused
           将“连接拒绝”视为暂时性错误，然后再试一次。
           通常，Wget在无法连接到URL时会放弃URL
           站点因为连接失败被视为服务器的标志
           根本没有运行，重试无济于事。这个选项
           用于镜像服务器趋于消失的不可靠站点
           在短时间内。

       --user =用户
       --password =密码
           指定FTP和的用户名和密码密码
           HTTP文件检索。可以使用。覆盖这些参数
           FTP连接的--ftp-user和--ftp-password选项
            -  HTTP连接的-http-user和--http-password选项。

       --ask密码
           为每个建立的连接提示密码。不可能是
           在使用--password时指定，因为它们是相互的
           独家。

       --no-IRI
           关闭国际化URI（IRI）支持。使用--iri来转动它
           上。默认情况下会激活IRI支持。

           您可以使用“iri”设置IRI支持的默认状态
           命令.wgetrc。可以从命令覆盖该设置
           线。

       编码--local-=编码
           强制Wget使用编码作为默认系统编码。那
           影响Wget如何转换从语言环境指定为参数的URL
           到UTF-8获得IRI支持。

           Wget使用函数“nl_langinfo（）”然后使用“CHARSET”
           获取语​​言环境的环境变量。如果失败，则使用ASCII。

           您可以使用“local_encoding”设置默认本地编码
           命令.wgetrc。可以从命令覆盖该设置
           线。

       编码--remote-=编码
           强制Wget使用编码作为默认的远程服务器编码。
           这会影响Wget如何转换远程文件中的URI
           在递归提取期间编码为UTF-8。这个选项只是
           对于IRI支持非常有用，用于解释非ASCII
           字符。

           对于HTTP，可以在HTTP“Content-Type”中找到远程编码
           标题和HTML“Content-Type http-equiv”元标记。

           您可以使用“remoteencoding”命令设置默认编码
           在.wgetrc。可以从命令行覆盖该设置。

       --unlink
           强制Wget取消链接文件而不是破坏现有文件。这个
           选项对于下载到具有硬链接的目录很有用。

   目录选项
       -nd
       --no-目录
           检索时不要创建目录层次结构
           递归。启用此选项后，将保存所有文件
           到当前目录，没有clobbering（如果名称显示
           不止一次，文件名将获得扩展名.n）。

       -X
       --force-目录
           与-nd ---相反的是创建目录层次结构，即使
           否则就不会创造一个。例如。 wget -x
           http://fly.srk.fer.hr/robots.txt会将下载的文件保存到
           fly.srk.fer.hr/robots.txt。

       -nH
       --no宿主目录
           禁用主机前缀目录的生成。默认情况下，
           使用-r http://fly.srk.fer.hr/调用Wget将创建一个
           以fly.srk.fer.hr/开头的目录结构。这个
           选项禁用此类行为。

       --protocol的目录
           使用协议名称作为本地文件名的目录组件。
           例如，使用此选项，wget -r http：// host将保存到
           http / host / ...而不仅仅是托管/ ....

       --Cut-迪尔斯=数
           忽略数字目录组件。这对获得一个很有用
           对递归检索的目录进行细粒度控制
           将被保存。

           以，例如，目录为
           ftp://ftp.xemacs.org/pub/xemacs/。如果你用-r检索它，它
           将在本地保存在ftp.xemacs.org/pub/xemacs/下。虽然
           -nH选项可以删除ftp.xemacs.org/部分，你仍然卡住了
           与pub / xemacs。这就是--cut-dirs派上用场的地方;它使
           Wget没有“看到”数字远程目录组件。这是
           有关-cut-dirs选项如何工作的几个例子。

                   没有选项 - > ftp.xemacs.org/pub/xemacs/
                   -nH  - > pub / xemacs /
                   -nH --cut-dirs = 1  - > xemacs /
                   -nH --cut-dirs = 2  - >。

                   --cut-dirs = 1  - > ftp.xemacs.org/xemacs/
                   ...

           如果你只是想摆脱目录结构，这个选项
           类似于-nd和-P的组合。但是，与-nd不同，
           --cut-dirs不会丢失子目录 - 例如，with
           -nH --cut-dirs = 1，将放置一个beta /子目录
           xemacs / beta，正如人们所期望的那样。

       -P前缀
       --directory前缀=前缀
           将目录前缀设置为前缀。目录前缀是
           将保存所有其他文件和子目录的目录
           到，即检索树的顶部。默认是。 （该
           当前目录）。

   HTTP选项
       --default页=名称
           当名称不知道时使用名称作为默认文件名（即for
           以斜杠结尾的网址，而不是index.html。

       -E
        - 调整延伸
           如果下载了application / xhtml + xml或text / html类型的文件
           并且URL不以regexp \。[Hh] [Tt] [mm] [Ll]？结尾，这个
           选项将导致后缀.html被附加到本地
           文件名。 This is useful, for instance, when you're mirroring a
           remote site that uses .asp pages, but you want the mirrored pages
           to be viewable on your stock Apache server.  Another good use for
           this is when you're downloading CGI-generated materials.  A URL
           like http://site.com/article.cgi?25 will be saved as
           article.cgi?25.html.

           Note that filenames changed in this way will be re-downloaded every
           time you re-mirror a site, because Wget can't tell that the local
           X.html file corresponds to remote URL X (since it doesn't yet know
           that the URL produces output of type text/html or
           application/xhtml+xml.

           As of version 1.12, Wget will also ensure that any downloaded files
           of type text/css end in the suffix .css, and the option was renamed
           from --html-extension, to better reflect its new behavior. The old
           option name is still acceptable, but should now be considered
           deprecated.

           At some point in the future, this option may well be expanded to
           include suffixes for other types of content, including content
           types that are not parsed by Wget.

       --http-user=user
       --http-password=password
           Specify the username user and password password on an HTTP server.
           According to the type of the challenge, Wget will encode them using
           either the "basic" (insecure), the "digest", or the Windows "NTLM"
           authentication scheme.

           Another way to specify username and password is in the URL itself.
           Either method reveals your password to anyone who bothers to run
           "ps".  To prevent the passwords from being seen, store them in
           .wgetrc or .netrc, and make sure to protect those files from other
           users with "chmod".  If the passwords are really important, do not
           leave them lying in those files either---edit the files and delete
           them after Wget has started the download.

       --no-http-keep-alive
           Turn off the "keep-alive" feature for HTTP downloads.  Normally,
           Wget asks the server to keep the connection open so that, when you
           download more than one document from the same server, they get
           transferred over the same TCP connection.  This saves time and at
           the same time reduces the load on the server.

           This option is useful when, for some reason, persistent (keep-
           alive) connections don't work for you, for example due to a server
           bug or due to the inability of server-side scripts to cope with the
           connections.

       --no-cache
           Disable server-side cache.  In this case, Wget will send the remote
           server an appropriate directive (Pragma: no-cache) to get the file
           from the remote service, rather than returning the cached version.
           This is especially useful for retrieving and flushing out-of-date
           documents on proxy servers.

           Caching is allowed by default.

       --no-cookies
           Disable the use of cookies.  Cookies are a mechanism for
           maintaining server-side state.  The server sends the client a
           cookie using the "Set-Cookie" header, and the client responds with
           the same cookie upon further requests.  Since cookies allow the
           server owners to keep track of visitors and for sites to exchange
           this information, some consider them a breach of privacy.该
           default is to use cookies; however, storing cookies is not on by
           default.

       --load-cookies file
           Load cookies from file before the first HTTP retrieval.  file is a
           textual file in the format originally used by Netscape's
           cookies.txt file.

           You will typically use this option when mirroring sites that
           require that you be logged in to access some or all of their
           content.  The login process typically works by the web server
           issuing an HTTP cookie upon receiving and verifying your
           credentials.  The cookie is then resent by the browser when
           accessing that part of the site, and so proves your identity.

           Mirroring such a site requires Wget to send the same cookies your
           browser sends when communicating with the site.  This is achieved
           by --load-cookies---simply point Wget to the location of the
           cookies.txt file, and it will send the same cookies your browser
           would send in the same situation.  Different browsers keep textual
           cookie files in different locations:

           "Netscape 4.x."
               The cookies are in ~/.netscape/cookies.txt.

           "Mozilla and Netscape 6.x."
               Mozilla's cookie file is also named cookies.txt, located
               somewhere under ~/.mozilla, in the directory of your profile.
               The full path usually ends up looking somewhat like
               ~/.mozilla/default/some-weird-string/cookies.txt.

           "Internet Explorer."
               You can produce a cookie file Wget can use by using the File
               menu, Import and Export, Export Cookies.  This has been tested
               with Internet Explorer 5; it is not guaranteed to work with
               earlier versions.

           "Other browsers."
               If you are using a different browser to create your cookies,
               --load-cookies will only work if you can locate or produce a
               cookie file in the Netscape format that Wget expects.

           If you cannot use --load-cookies, there might still be an
           alternative.  If your browser supports a "cookie manager", you can
           use it to view the cookies used when accessing the site you're
           mirroring.  Write down the name and value of the cookie, and
           manually instruct Wget to send those cookies, bypassing the
           "official" cookie support:

                   wget --no-cookies --header "Cookie: <name>=<value>"

       --save-cookies file
           Save cookies to file before exiting.  This will not save cookies
           that have expired or that have no expiry time (so-called "session
           cookies"), but also see --keep-session-cookies.

       --keep-session-cookies
           When specified, causes --save-cookies to also save session cookies.
           Session cookies are normally not saved because they are meant to be
           kept in memory and forgotten when you exit the browser.  Saving
           them is useful on sites that require you to log in or to visit the
           home page before you can access some pages.  With this option,
           multiple Wget runs are considered a single browser session as far
           as the site is concerned.

           Since the cookie file format does not normally carry session
           cookies, Wget marks them with an expiry timestamp of 0.  Wget's
           --load-cookies recognizes those as session cookies, but it might
           confuse other browsers.  Also note that cookies so loaded will be
           treated as other session cookies, which means that if you want
           --save-cookies to preserve them again, you must use
           --keep-session-cookies again.

       --ignore-length
           Unfortunately, some HTTP servers (CGI programs, to be more precise)
           send out bogus "Content-Length" headers, which makes Wget go wild,
           as it thinks not all the document was retrieved.  You can spot this
           syndrome if Wget retries getting the same document again and again,
           each time claiming that the (otherwise normal) connection has
           closed on the very same byte.

           With this option, Wget will ignore the "Content-Length" header---as
           if it never existed.

       --header=header-line
           Send header-line along with the rest of the headers in each HTTP
           请求。 The supplied header is sent as-is, which means it must
           contain name and value separated by colon, and must not contain
           newlines.

           You may define more than one additional header by specifying
           --header more than once.

                   wget --header='Accept-Charset: iso-8859-2' \
                        --header='Accept-Language: hr'        \
                          http://fly.srk.fer.hr/

           Specification of an empty string as the header value will clear all
           previous user-defined headers.

           As of Wget 1.10, this option can be used to override headers
           otherwise generated automatically.  This example instructs Wget to
           connect to localhost, but to specify foo.bar in the "Host" header:

                   wget --header="Host: foo.bar" http://localhost/

           In versions of Wget prior to 1.10 such use of --header caused
           sending of duplicate headers.

       --max-redirect=number
           Specifies the maximum number of redirections to follow for a
           resource.  The default is 20, which is usually far more than
           necessary. However, on those occasions where you want to allow more
           (or fewer), this is the option to use.

       --proxy-user=user
       --proxy-password=password
           Specify the username user and password password for authentication
           on a proxy server.  Wget will encode them using the "basic"
           authentication scheme.

           Security considerations similar to those with --http-password
           pertain here as well.

       --referer=url
           Include `Referer: url' header in HTTP request.  Useful for
           retrieving documents with server-side processing that assume they
           are always being retrieved by interactive web browsers and only
           come out properly when Referer is set to one of the pages that
           point to them.

       --save-headers
           Save the headers sent by the HTTP server to the file, preceding the
           actual contents, with an empty line as the separator.

       -U agent-string
       --user-agent=agent-string
           Identify as agent-string to the HTTP server.

           The HTTP protocol allows the clients to identify themselves using a
           "User-Agent" header field.  This enables distinguishing the WWW
           software, usually for statistical purposes or for tracing of
           protocol violations.  Wget normally identifies as Wget/version,
           version being the current version number of Wget.

           However, some sites have been known to impose the policy of
           tailoring the output according to the "User-Agent"-supplied
           information.  While this is not such a bad idea in theory, it has
           been abused by servers denying information to clients other than
           (historically) Netscape or, more frequently, Microsoft Internet
           Explorer.  This option allows you to change the "User-Agent" line
           issued by Wget.  Use of this option is discouraged, unless you
           really know what you are doing.

           Specifying empty user agent with --user-agent="" instructs Wget not
           to send the "User-Agent" header in HTTP requests.

       --post-data=string
       --post-file=file
           Use POST as the method for all HTTP requests and send the specified
           data in the request body.  --post-data sends string as data,
           whereas --post-file sends the contents of file.  Other than that,
           they work in exactly the same way. In particular, they both expect
           content of the form "key1=value1&key2=value2", with percent-
           encoding for special characters; the only difference is that one
           expects its content as a command-line parameter and the other
           accepts its content from a file. In particular, --post-file is not
           for transmitting files as form attachments: those must appear as
           "key=value" data (with appropriate percent-coding) just like
           everything else. Wget does not currently support
           "multipart/form-data" for transmitting POST data;只要
           "application/x-www-form-urlencoded". Only one of --post-data and
           --post-file should be specified.

           Please be aware that Wget needs to know the size of the POST data
           in advance.  Therefore the argument to "--post-file" must be a
           regular file; specifying a FIFO or something like /dev/stdin won't
           工作。 It's not quite clear how to work around this limitation
           inherent in HTTP/1.0.  Although HTTP/1.1 introduces chunked
           transfer that doesn't require knowing the request length in
           advance, a client can't use chunked unless it knows it's talking to
           an HTTP/1.1 server.  And it can't know that until it receives a
           response, which in turn requires the request to have been completed
           -- a chicken-and-egg problem.

           Note: if Wget is redirected after the POST request is completed, it
           will not send the POST data to the redirected URL.  This is because
           URLs that process POST often respond with a redirection to a
           regular page, which does not desire or accept POST.它不是
           completely clear that this behavior is optimal;如果它不起作用
           out, it might be changed in the future.

           This example shows how to log to a server using POST and then
           proceed to download the desired pages, presumably only accessible
           to authorized users:

                   # Log in to the server.  This can be done only once.
                   wget --save-cookies cookies.txt \
                        --post-data 'user=foo&password=bar' \
                        http://server.com/auth.php

                   # Now grab the page or pages we care about.
                   wget --load-cookies cookies.txt \
                        -p http://server.com/interesting/article.php

           If the server is using session cookies to track user
           authentication, the above will not work because --save-cookies will
           not save them (and neither will browsers) and the cookies.txt file
           will be empty.  In that case use --keep-session-cookies along with
           --save-cookies to force saving of session cookies.

       --content-disposition
           If this is set to on, experimental (not fully-functional) support
           for "Content-Disposition" headers is enabled. This can currently
           result in extra round-trips to the server for a "HEAD" request, and
           is known to suffer from a few bugs, which is why it is not
           currently enabled by default.

           This option is useful for some file-downloading CGI programs that
           use "Content-Disposition" headers to describe what the name of a
           downloaded file should be.

       --content-on-error
           If this is set to on, wget will not skip the content when the
           server responds with a http status code that indicates error.

       --trust-server-names
           If this is set to on, on a redirect the last component of the
           redirection URL will be used as the local file name.  By default it
           is used the last component in the original URL.

       --auth-no-challenge
           If this option is given, Wget will send Basic HTTP authentication
           information (plaintext username and password) for all requests,
           just like Wget 1.10.2 and prior did by default.

           Use of this option is not recommended, and is intended only to
           support some few obscure servers, which never send HTTP
           authentication challenges, but accept unsolicited auth info, say,
           in addition to form-based authentication.

   HTTPS (SSL/TLS) Options
       To support encrypted HTTP (HTTPS) downloads, Wget must be compiled with
       an external SSL library, currently OpenSSL.  If Wget is compiled
       without SSL support, none of these options are available.

       --secure-protocol=protocol
           Choose the secure protocol to be used.  Legal values are auto,
           SSLv2, SSLv3, TLSv1, TLSv1_1 and TLSv1_2.  If auto is used, the SSL
           library is given the liberty of choosing the appropriate protocol
           automatically, which is achieved by sending a SSLv2 greeting and
           announcing support for SSLv3 and TLSv1.这是默认值。

           Specifying SSLv2, SSLv3, TLSv1, TLSv1_1 or TLSv1_2 forces the use
           of the corresponding protocol.  This is useful when talking to old
           and buggy SSL server implementations that make it hard for the
           underlying SSL library to choose the correct protocol version.
           Fortunately, such servers are quite rare.

       --no-check-certificate
           Don't check the server certificate against the available
           certificate authorities.  Also don't require the URL host name to
           match the common name presented by the certificate.

           As of Wget 1.10, the default is to verify the server's certificate
           against the recognized certificate authorities, breaking the SSL
           handshake and aborting the download if the verification fails.
           Although this provides more secure downloads, it does break
           interoperability with some sites that worked with previous Wget
           versions, particularly those using self-signed, expired, or
           otherwise invalid certificates.  This option forces an "insecure"
           mode of operation that turns the certificate verification errors
           into warnings and allows you to proceed.

           If you encounter "certificate verification" errors or ones saying
           that "common name doesn't match requested host name", you can use
           this option to bypass the verification and proceed with the
           download.  Only use this option if you are otherwise convinced of
           the site's authenticity, or if you really don't care about the
           validity of its certificate.  It is almost always a bad idea not to
           check the certificates when transmitting confidential or important
           data.

       --certificate=file
           Use the client certificate stored in file.  This is needed for
           servers that are configured to require certificates from the
           clients that connect to them.  Normally a certificate is not
           required and this switch is optional.

       --certificate-type=type
           Specify the type of the client certificate.  Legal values are PEM
           (assumed by default) and DER, also known as ASN1.

       --private-key=file
           Read the private key from file.  This allows you to provide the
           private key in a file separate from the certificate.

       --private-key-type=type
           Specify the type of the private key.  Accepted values are PEM (the
           default) and DER.

       --ca-certificate=file
           Use file as the file with the bundle of certificate authorities
           ("CA") to verify the peers.  The certificates must be in PEM
           格式。

           Without this option Wget looks for CA certificates at the system-
           specified locations, chosen at OpenSSL installation time.

       --ca-directory=directory
           Specifies directory containing CA certificates in PEM format.  Each
           file contains one CA certificate, and the file name is based on a
           hash value derived from the certificate.  This is achieved by
           processing a certificate directory with the "c_rehash" utility
           supplied with OpenSSL.  Using --ca-directory is more efficient than
           --ca-certificate when many certificates are installed because it
           allows Wget to fetch certificates on demand.

           Without this option Wget looks for CA certificates at the system-
           specified locations, chosen at OpenSSL installation time.

       --random-file=file
           Use file as the source of random data for seeding the pseudo-random
           number generator on systems without /dev/random.

           On such systems the SSL library needs an external source of
           randomness to initialize.  Randomness may be provided by EGD (see
           --egd-file below) or read from an external source specified by the
           user.  If this option is not specified, Wget looks for random data
           in $RANDFILE or, if that is unset, in $HOME/.rnd.  If none of those
           are available, it is likely that SSL encryption will not be usable.

           If you're getting the "Could not seed OpenSSL PRNG; disabling SSL."
           error, you should provide random data using some of the methods
           described above.

       --egd-file=file
           Use file as the EGD socket.  EGD stands for Entropy Gathering
           Daemon, a user-space program that collects data from various
           unpredictable system sources and makes it available to other
           programs that might need it.  Encryption software, such as the SSL
           library, needs sources of non-repeating randomness to seed the
           random number generator used to produce cryptographically strong
           keys.

           OpenSSL allows the user to specify his own source of entropy using
           the "RAND_FILE" environment variable.  If this variable is unset,
           or if the specified file does not produce enough randomness,
           OpenSSL will read random data from EGD socket specified using this
           option.

           If this option is not specified (and the equivalent startup command
           is not used), EGD is never contacted.  EGD is not needed on modern
           Unix systems that support /dev/random.

       --warc-file=file
           Use file as the destination WARC file.

       --warc-header=string
           Use string into as the warcinfo record.

       --warc-max-size=size
           Set the maximum size of the WARC files to size.

       --warc-cdx
           Write CDX index files.

       --warc-dedup=file
           Do not store records listed in this CDX file.

       --no-warc-compression
           Do not compress WARC files with GZIP.

       --no-warc-digests
           Do not calculate SHA1 digests.

       --no-warc-keep-log
           Do not store the log file in a WARC record.

       --warc-tempdir=dir
           Specify the location for temporary files created by the WARC
           writer.

   FTP Options
       --ftp-user=user
       --ftp-password=password
           Specify the username user and password password on an FTP server.
           Without this, or the corresponding startup option, the password
           defaults to -wget@, normally used for anonymous FTP.

           Another way to specify username and password is in the URL itself.
           Either method reveals your password to anyone who bothers to run
           "ps".  To prevent the passwords from being seen, store them in
           .wgetrc or .netrc, and make sure to protect those files from other
           users with "chmod".  If the passwords are really important, do not
           leave them lying in those files either---edit the files and delete
           them after Wget has started the download.

       --no-remove-listing
           Don't remove the temporary .listing files generated by FTP
           retrievals.  Normally, these files contain the raw directory
           listings received from FTP servers.  Not removing them can be
           useful for debugging purposes, or when you want to be able to
           easily check on the contents of remote server directories (e.g. to
           verify that a mirror you're running is complete).

           Note that even though Wget writes to a known filename for this
           file, this is not a security hole in the scenario of a user making
           .listing a symbolic link to /etc/passwd or something and asking
           "root" to run Wget in his or her directory.  Depending on the
           options used, either Wget will refuse to write to .listing, making
           the globbing/recursion/time-stamping operation fail, or the
           symbolic link will be deleted and replaced with the actual .listing
           file, or the listing will be written to a .listing.number file.

           Even though this situation isn't a problem, though, "root" should
           never run Wget in a non-trusted user's directory.  A user could do
           something as simple as linking index.html to /etc/passwd and asking
           "root" to run Wget with -N or -r so the file will be overwritten.

       --no-glob
           Turn off FTP globbing.  Globbing refers to the use of shell-like
           special characters (wildcards), like *, ?, [ and ] to retrieve more
           than one file from the same directory at once, like:

                   wget ftp://gnjilux.srk.fer.hr/*.msg

           By default, globbing will be turned on if the URL contains a
           globbing character.  This option may be used to turn globbing on or
           off permanently.

           You may have to quote the URL to protect it from being expanded by
           your shell.  Globbing makes Wget look for a directory listing,
           which is system-specific.  This is why it currently works only with
           Unix FTP servers (and the ones emulating Unix "ls" output).

       --no-passive-ftp
           Disable the use of the passive FTP transfer mode.  Passive FTP
           mandates that the client connect to the server to establish the
           data connection rather than the other way around.

           If the machine is connected to the Internet directly, both passive
           and active FTP should work equally well.  Behind most firewall and
           NAT configurations passive FTP has a better chance of working.
           However, in some rare firewall configurations, active FTP actually
           works when passive FTP doesn't.  If you suspect this to be the
           case, use this option, or set "passive_ftp=off" in your init file.

       --preserve-permissions
           Preserve remote file permissions instead of permissions set by
           umask.

       --retr-symlinks
           By default, when retrieving FTP directories recursively and a
           symbolic link is encountered, the symbolic link is traversed and
           the pointed-to files are retrieved.  Currently, Wget does not
           traverse symbolic links to directories to download them
           recursively, though this feature may be added in the future.

           When --retr-symlinks=no is specified, the linked-to file is not
           downloaded.  Instead, a matching symbolic link is created on the
           local filesystem.  The pointed-to file will not be retrieved unless
           this recursive retrieval would have encountered it separately and
           downloaded it anyway.  This option poses a security risk where a
           malicious FTP Server may cause Wget to write to files outside of
           the intended directories through a specially crafted .LISTING file.

           Note that when retrieving a file (not a directory) because it was
           specified on the command-line, rather than because it was recursed
           to, this option has no effect.  Symbolic links are always traversed
           in this case.

   Recursive Retrieval Options
       -r
       --recursive
           Turn on recursive retrieving.    The default maximum depth is 5.

       -l depth
       --level=depth
           Specify recursion maximum depth level depth.

       --delete-after
           This option tells Wget to delete every single file it downloads,
           after having done so.  It is useful for pre-fetching popular pages
           through a proxy, e.g.:

                   wget -r -nd --delete-after http://whatever.com/~popular/page/

           The -r option is to retrieve recursively, and -nd to not create
           directories.

           Note that --delete-after deletes files on the local machine.它
           does not issue the DELE command to remote FTP sites, for instance.
           Also note that when --delete-after is specified, --convert-links is
           ignored, so .orig files are simply not created in the first place.

       -k
       --convert-links
           After the download is complete, convert the links in the document
           to make them suitable for local viewing.  This affects not only the
           visible hyperlinks, but any part of the document that links to
           external content, such as embedded images, links to style sheets,
           hyperlinks to non-HTML content, etc.

           Each link will be changed in one of the two ways:

           Â·   The links to files that have been downloaded by Wget will be
               changed to refer to the file they point to as a relative link.

               Example: if the downloaded file /foo/doc.html links to
               /bar/img.gif, also downloaded, then the link in doc.html will
               be modified to point to ../bar/img.gif.  This kind of
               transformation works reliably for arbitrary combinations of
               directories.

           Â·   The links to files that have not been downloaded by Wget will
               be changed to include host name and absolute path of the
               location they point to.

               Example: if the downloaded file /foo/doc.html links to
               /bar/img.gif (or to ../bar/img.gif), then the link in doc.html
               will be modified to point to http://hostname/bar/img.gif.

           Because of this, local browsing works reliably: if a linked file
           was downloaded, the link will refer to its local name; if it was
           not downloaded, the link will refer to its full Internet address
           rather than presenting a broken link.  The fact that the former
           links are converted to relative links ensures that you can move the
           downloaded hierarchy to another directory.

           Note that only at the end of the download can Wget know which links
           have been downloaded.  Because of that, the work done by -k will be
           performed at the end of all the downloads.

       -K
       --backup-converted
           When converting a file, back up the original version with a .orig
           suffix.  Affects the behavior of -N.

       -m
       --mirror
           Turn on options suitable for mirroring.  This option turns on
           recursion and time-stamping, sets infinite recursion depth and
           keeps FTP directory listings.  It is currently equivalent to -r -N
           -l inf --no-remove-listing.

       -p
       --page-requisites
           This option causes Wget to download all the files that are
           necessary to properly display a given HTML page.  This includes
           such things as inlined images, sounds, and referenced stylesheets.

           Ordinarily, when downloading a single HTML page, any requisite
           documents that may be needed to display it properly are not
           downloaded.  Using -r together with -l can help, but since Wget
           does not ordinarily distinguish between external and inlined
           documents, one is generally left with "leaf documents" that are
           missing their requisites.

           For instance, say document 1.html contains an "<IMG>" tag
           referencing 1.gif and an "<A>" tag pointing to external document
           2.html.  Say that 2.html is similar but that its image is 2.gif and
           it links to 3.html.  Say this continues up to some arbitrarily high
           number.

           If one executes the command:

                   wget -r -l 2 http://<site>/1.html

           then 1.html, 1.gif, 2.html, 2.gif, and 3.html will be downloaded.
           As you can see, 3.html is without its requisite 3.gif because Wget
           is simply counting the number of hops (up to 2) away from 1.html in
           order to determine where to stop the recursion.  However, with this
           command:

                   wget -r -l 2 -p http://<site>/1.html

           all the above files and 3.html's requisite 3.gif will be
           downloaded.  Similarly,

                   wget -r -l 1 -p http://<site>/1.html

           will cause 1.html, 1.gif, 2.html, and 2.gif to be downloaded.  One
           might think that:

                   wget -r -l 0 -p http://<site>/1.html

           would download just 1.html and 1.gif, but unfortunately this is not
           the case, because -l 0 is equivalent to -l inf---that is, infinite
           recursion.  To download a single HTML page (or a handful of them,
           all specified on the command-line or in a -i URL input file) and
           its (or their) requisites, simply leave off -r and -l:

                   wget -p http://<site>/1.html

           Note that Wget will behave as if -r had been specified, but only
           that single page and its requisites will be downloaded.  Links from
           that page to external documents will not be followed.  Actually, to
           download a single page and all its requisites (even if they exist
           on separate websites), and make sure the lot displays properly
           locally, this author likes to use a few options in addition to -p:

                   wget -E -H -k -K -p http://<site>/<document>

           To finish off this topic, it's worth knowing that Wget's idea of an
           external document link is any URL specified in an "<A>" tag, an
           "<AREA>" tag, or a "<LINK>" tag other than "<LINK
           REL="stylesheet">".

       --strict-comments
           Turn on strict parsing of HTML comments.  The default is to
           terminate comments at the first occurrence of -->.

           According to specifications, HTML comments are expressed as SGML
           declarations.  Declaration is special markup that begins with <!
           and ends with >, such as <!DOCTYPE ...>, that may contain comments
           between a pair of -- delimiters.  HTML comments are "empty
           declarations", SGML declarations without any non-comment text.
           Therefore, <!--foo--> is a valid comment, and so is <!--one--
           --two-->, but <!--1--2--> is not.

           On the other hand, most HTML writers don't perceive comments as
           anything other than text delimited with <!-- and -->, which is not
           quite the same.  For example, something like <!------------> works
           as a valid comment as long as the number of dashes is a multiple of
           four (!).  If not, the comment technically lasts until the next --,
           which may be at the other end of the document.  Because of this,
           many popular browsers completely ignore the specification and
           implement what users have come to expect: comments delimited with
           <!-- and -->.

           Until version 1.9, Wget interpreted comments strictly, which
           resulted in missing links in many web pages that displayed fine in
           browsers, but had the misfortune of containing non-compliant
           comments.  Beginning with version 1.9, Wget has joined the ranks of
           clients that implements "naive" comments, terminating each comment
           at the first occurrence of -->.

           If, for whatever reason, you want strict comment parsing, use this
           option to turn it on.

   Recursive Accept/Reject Options
       -A acclist --accept acclist
       -R rejlist --reject rejlist
           Specify comma-separated lists of file name suffixes or patterns to
           accept or reject. Note that if any of the wildcard characters, *,
           ?, [ or ], appear in an element of acclist or rejlist, it will be
           treated as a pattern, rather than a suffix.

       --accept-regex urlregex
       --reject-regex urlregex
           Specify a regular expression to accept or reject the complete URL.

       --regex-type regextype
           Specify the regular expression type.  Possible types are posix or
           pcre.  Note that to be able to use pcre type, wget has to be
           compiled with libpcre support.

       -D domain-list
       --domains=domain-list
           Set domains to be followed.  domain-list is a comma-separated list
           of domains.  Note that it does not turn on -H.

       --exclude-domains domain-list
           Specify the domains that are not to be followed.

       --follow-ftp
           Follow FTP links from HTML documents.  Without this option, Wget
           will ignore all the FTP links.

       --follow-tags=list
           Wget has an internal table of HTML tag / attribute pairs that it
           considers when looking for linked documents during a recursive
           retrieval.  If a user wants only a subset of those tags to be
           considered, however, he or she should be specify such tags in a
           comma-separated list with this option.

       --ignore-tags=list
           This is the opposite of the --follow-tags option.  To skip certain
           HTML tags when recursively looking for documents to download,
           specify them in a comma-separated list.

           In the past, this option was the best bet for downloading a single
           page and its requisites, using a command-line like:

                   wget --ignore-tags=a,area -H -k -K -r http://<site>/<document>

           However, the author of this option came across a page with tags
           like "<LINK REL="home" HREF="/">" and came to the realization that
           specifying tags to ignore was not enough.  One can't just tell Wget
           to ignore "<LINK>", because then stylesheets will not be
           downloaded.  Now the best bet for downloading a single page and its
           requisites is the dedicated --page-requisites option.

       --ignore-case
           Ignore case when matching files and directories.  This influences
           the behavior of -R, -A, -I, and -X options, as well as globbing
           implemented when downloading from FTP sites.  For example, with
           this option, -A *.txt will match file1.txt, but also file2.TXT,
           file3.TxT, and so on.

       -H
       --span-hosts
           Enable spanning across hosts when doing recursive retrieving.

       -L
       --relative
           Follow relative links only.  Useful for retrieving a specific home
           page without any distractions, not even those from the same hosts.

       -I list
       --include-directories=list
           Specify a comma-separated list of directories you wish to follow
           when downloading.  Elements of list may contain wildcards.

       -X list
       --exclude-directories=list
           Specify a comma-separated list of directories you wish to exclude
           from download.  Elements of list may contain wildcards.

       -np
       --no-parent
           Do not ever ascend to the parent directory when retrieving
           recursively.  This is a useful option, since it guarantees that
           only the files below a certain hierarchy will be downloaded.

环境
       Wget supports proxies for both HTTP and FTP retrievals.  The standard
       way to specify proxy location, which Wget recognizes, is using the
       following environment variables:

       http_proxy
       https_proxy
           If set, the http_proxy and https_proxy variables should contain the
           URLs of the proxies for HTTP and HTTPS connections respectively.

       ftp_proxy
           This variable should contain the URL of the proxy for FTP
           connections.  It is quite common that http_proxy and ftp_proxy are
           set to the same URL.

       no_proxy
           This variable should contain a comma-separated list of domain
           extensions proxy should not be used for.  For instance, if the
           value of no_proxy is .mit.edu, proxy will not be used to retrieve
           documents from MIT.

退出状态
       Wget may return one of several error codes if it encounters problems.

       0   No problems occurred.

       1   Generic error code.

       2   Parse error---for instance, when parsing command-line options, the
           .wgetrc or .netrc...

       3   File I/O error.

       4   Network failure.

       5   SSL verification failure.

       6   Username/password authentication failure.

       7   Protocol errors.

       8   Server issued an error response.

       With the exceptions of 0 and 1, the lower-numbered exit codes take
       precedence over higher-numbered ones, when multiple types of errors are
       encountered.

       In versions of Wget prior to 1.12, Wget's exit status tended to be
       unhelpful and inconsistent. Recursive downloads would virtually always
       return 0 (success), regardless of any issues encountered, and non-
       recursive fetches only returned the status corresponding to the most
       recently-attempted download.

FILES
       /etc/wgetrc
           Default location of the global startup file.

       .wgetrc
           User startup file.

BUGS
       You are welcome to submit bug reports via the GNU Wget bug tracker (see
       <http://wget.addictivecode.org/BugTracker>).

       Before actually submitting a bug report, please try to follow a few
       simple guidelines.

       1.  Please try to ascertain that the behavior you see really is a bug.
           If Wget crashes, it's a bug.  If Wget does not behave as
           documented, it's a bug.  If things work strange, but you are not
           sure about the way they are supposed to work, it might well be a
           bug, but you might want to double-check the documentation and the
           mailing lists.

       2.  Try to repeat the bug in as simple circumstances as possible.例如。
           if Wget crashes while downloading wget -rl0 -kKE -t5 --no-proxy
           http://yoyodyne.com -o /tmp/log, you should try to see if the crash
           is repeatable, and if will occur with a simpler set of options.
           You might even try to start the download at the page where the
           crash occurred to see if that page somehow triggered the crash.

           Also, while I will probably be interested to know the contents of
           your .wgetrc file, just dumping it into the debug message is
           probably a bad idea.  Instead, you should first try to see if the
           bug repeats with .wgetrc moved out of the way.  Only if it turns
           out that .wgetrc settings affect the bug, mail me the relevant
           parts of the file.

       3.  Please start Wget with -d option and send us the resulting output
           (or relevant parts thereof).  If Wget was compiled without debug
           support, recompile it---it is much easier to trace bugs with debug
           support on.

           Note: please make sure to remove any potentially sensitive
           information from the debug log before sending it to the bug
           address.  The "-d" won't go out of its way to collect sensitive
           information, but the log will contain a fairly complete transcript
           of Wget's communication with the server, which may include
           passwords and pieces of downloaded data.  Since the bug address is
           publically archived, you may assume that all bug reports are
           visible to the public.

       4.  If Wget has crashed, try to run it in a debugger, e.g. "gdb `which
           wget` core" and type "where" to get the backtrace.  This may not
           work if the system administrator has disabled core files, but it is
           safe to try.

也可以看看
       This is not the complete manual for GNU Wget.  For more complete
       information, including more detailed explanations of some of the
       options, and a number of commands available for use with .wgetrc files
       and the -e option, see the GNU Info entry for wget.

作者
       Originally written by Hrvoje Niksic <hniksic@xemacs.org>.

版权
       Copyright (c) 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004,
       2005, 2006, 2007, 2008, 2009, 2010, 2011 Free Software Foundation, Inc.

       允许复制，分发和/或修改本文档
       under the terms of the GNU Free Documentation License, Version 1.2 or
       自由软件基金会发布的任何更新版本;没有
       Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.一个
       copy of the license is included in the section entitled "GNU Free
       Documentation License".



GNU Wget 1.14                     2017-08-04                           WGET(1)
